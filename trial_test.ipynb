{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from snorkel.labeling.model import LabelModel, MajorityLabelVoter\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def reorder_columns(df, cols_in_front):\n",
    "    \"\"\"Reorder columns in a pandas dataframe so that the columns in cols_in_front are in front.\n",
    "    \"\"\"\n",
    "    columns = list(df.columns)\n",
    "    for col in cols_in_front:\n",
    "        columns.remove(col)\n",
    "    columns = cols_in_front + columns\n",
    "    return df[columns]\n",
    "\n",
    "def lf_results_reported(path='./CITT/'):\n",
    "    df = pd.read_csv(path + 'calculated_values.txt', sep='|', low_memory=False)\n",
    "    df['lf'] = df['were_results_reported'] == 't'\n",
    "    df['lf'] = df['lf'].astype('int')\n",
    "    df = reorder_columns(df, ['nct_id', 'lf'])\n",
    "    return df\n",
    "\n",
    "def lf_num_sponsors(path='./CITT/'):\n",
    "    df = pd.read_csv(path + 'sponsors.txt', sep='|')\n",
    "    df = df.groupby('nct_id')['name'].count().reset_index()\n",
    "    df['lf'] = df['name'] > df['name'].quantile(.5)\n",
    "    df['lf'] = df['lf'].fillna(-1).astype('int')\n",
    "    df = reorder_columns(df, ['nct_id', 'lf'])\n",
    "    return df\n",
    "\n",
    "def lf_num_patients(path='./CITT/'):\n",
    "    df = pd.read_csv(path + 'outcome_counts.txt', sep='|', low_memory=False)    \n",
    "    df = df.groupby('nct_id').sum().reset_index() # pd df (NCTID, values, num_patients)\n",
    "    df['lf'] = df['count'] > df['count'].quantile(.5)\n",
    "    df['lf'] = df['lf'].fillna(-1).astype('int')\n",
    "    df = reorder_columns(df, ['nct_id', 'lf'])\n",
    "    return df\n",
    "\n",
    "def lf_patient_drop(path='./CITT/'):\n",
    "    # patient dropout\n",
    "    df = pd.read_csv(os.path.join(path, 'drop_withdrawals.txt'), sep='|')\n",
    "    df = df.groupby('nct_id').sum().reset_index() # pd df (NCTID, values, patient_drop)\n",
    "    df['lf'] = df['count'] < df['count'].quantile(.5)\n",
    "    df['lf'] = df['lf'].fillna(-1).astype('int')\n",
    "    df = reorder_columns(df, ['nct_id', 'lf'])\n",
    "    return df\n",
    "\n",
    "def lf_sites(path='./CITT/'):\n",
    "    # sites\n",
    "    df = pd.read_csv(os.path.join(path, 'facilities.txt'), sep='|')\n",
    "    df = df.groupby('nct_id')['name'].count().sort_values(ascending=False).reset_index()\n",
    "    df = df.groupby('nct_id').mean().reset_index() # pd df (NCTID, values, sites)\n",
    "    df['lf'] = df['name'] > df['name'].quantile(.5)\n",
    "    df['lf'] = df['lf'].fillna(-1).astype('int')\n",
    "    df = reorder_columns(df, ['nct_id', 'lf'])\n",
    "    return df\n",
    "\n",
    "def lf_pvalues(path='./CITT/'):\n",
    "    # pvalues\n",
    "    path = './CITT/'\n",
    "    df = pd.read_csv(os.path.join(path, 'outcome_analyses.txt'), sep='|', low_memory=False)\n",
    "    df['lf'] = df['p_value'] < .05 # 89406\n",
    "    df = df.groupby('nct_id')[['lf', 'p_value']].mean().reset_index() # multiple pvalues per nct_id\n",
    "    df['lf'] = df['lf'] > df['lf'].quantile(.5) \n",
    "    df['lf'] = df['lf'].fillna(-1).astype('int')\n",
    "    df = reorder_columns(df, ['nct_id', 'lf'])\n",
    "    return df\n",
    "\n",
    "def lf_update_more_recent(path='./CITT/'): #TODO clarify what this does\n",
    "    df = pd.read_csv(os.path.join(path, 'studies.txt'), sep='|', low_memory=False)\n",
    "    df['last_update_submitted_date'] = pd.to_datetime(df['last_update_submitted_date'])\n",
    "    df['completion_date'] = pd.to_datetime(df['completion_date'])\n",
    "    df['update_days'] = (df['last_update_submitted_date'] - df['completion_date']).dt.days\n",
    "    median = df['update_days'].quantile(.5) \n",
    "    # print(median)\n",
    "    df['lf'] = df['update_days'].apply(lambda x: x < median if pd.notna(x) else x)\n",
    "    df['lf'] = df['lf'].fillna(-1).astype('int')\n",
    "    df = reorder_columns(df, ['nct_id', 'lf']) \n",
    "    return df\n",
    "\n",
    "def lf_death_ae(path='./CITT/'):\n",
    "    df = pd.read_csv(path+'reported_event_totals.txt', sep = '|')\n",
    "    df = df[df['event_type'] == 'deaths'].fillna(0)\n",
    "    df = df.groupby('nct_id')['subjects_affected'].sum().reset_index()\n",
    "    df['lf'] = df['subjects_affected'] <= df['subjects_affected'].quantile(.5)\n",
    "    df['lf'] = df['lf'].fillna(-1).astype('int')\n",
    "    df = reorder_columns(df, ['nct_id', 'lf'])\n",
    "    return df\n",
    "\n",
    "def lf_serious_ae(path='./CITT/'):\n",
    "    df = pd.read_csv(path+'reported_event_totals.txt', sep = '|')\n",
    "    df = df[df['event_type'] == 'serious'].fillna(0)\n",
    "    df = df.groupby('nct_id')['subjects_affected'].sum().reset_index()\n",
    "    df['lf'] = df['subjects_affected'] <= df['subjects_affected'].quantile(.5)\n",
    "    df['lf'] = df['lf'].fillna(-1).astype('int')\n",
    "    df = reorder_columns(df, ['nct_id', 'lf'])\n",
    "    return df\n",
    "\n",
    "def lf_all_ae(path='./CITT/'):\n",
    "    df = pd.read_csv(path+'reported_event_totals.txt', sep = '|').fillna(0)\n",
    "    df = df.groupby('nct_id')['subjects_affected'].sum().reset_index()\n",
    "    df['lf'] = df['subjects_affected'] <= df['subjects_affected'].quantile(.5)\n",
    "    df['lf'] = df['lf'].fillna(-1).astype('int')\n",
    "    df = reorder_columns(df, ['nct_id', 'lf'])\n",
    "    return df\n",
    "\n",
    "def lf_status(path='./CITT/'):\n",
    "    df = pd.read_csv(path+'studies.txt', sep='|')\n",
    "    df['lf'] = -1\n",
    "    df.loc[df['overall_status'].isin(['Terminated', 'Withdrawn', 'Suspended', 'Withheld', 'No longer available', 'Temporarily not available']),['lf']] = 0\n",
    "    df.loc[df['overall_status'].isin(['Approved for marketing']),['lf']] = 1\n",
    "    df['lf'] = df['lf'].fillna(-1).astype('int')\n",
    "    df = reorder_columns(df, ['nct_id', 'lf'])\n",
    "    return df\n",
    "\n",
    "def lf_amendments(path='./stock_price/labels_and_tickers.csv'):\n",
    "    df = pd.read_csv(path)\n",
    "    df['lf'] = df['amendment_counts'] > df['amendment_counts'].quantile(.5)\n",
    "    df['lf'] = df['lf'].fillna(-1).astype('int')\n",
    "    df = reorder_columns(df, ['nct_id', 'lf'])\n",
    "    return df\n",
    "    \n",
    "def lf_stock_price(path='./stock_price/labels_and_tickers.csv'):\n",
    "    df = pd.read_csv(path)\n",
    "    df['lf'] = df['Slope'] > 0\n",
    "    df['lf'] = df['lf'].fillna(-1).astype('int')\n",
    "    df = reorder_columns(df, ['nct_id', 'lf'])\n",
    "    return df\n",
    "    \n",
    "def lf_linkage(path='./Trial Linkage/Merged_(ALL)_trial_linkage_outcome_df_FDA_updated.csv'):\n",
    "    df = pd.read_csv(path)\n",
    "    df.rename(columns={'nctid': 'nct_id'}, inplace=True)\n",
    "    df['lf'] = 0\n",
    "    # df.loc[df['outcome']=='Not sure',['lf']] = 1\n",
    "    # df.loc[df['outcome']=='Not sure',['lf']] = -1\n",
    "    df.loc[df['outcome']=='Not sure',['lf']] = 0\n",
    "    df.loc[df['outcome']=='Success', ['lf']] = 1\n",
    "    df = reorder_columns(df, ['nct_id', 'lf'])\n",
    "    return df\n",
    "\n",
    "def lf_news_headlines(path='./stock_price/studies_with_news.csv'):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.fillna(0)\n",
    "    sums = df['top_1_sim'] + df['top_2_sim'] + df['top_3_sim']\n",
    "    df['lf'] = sums > sums.quantile(.5)\n",
    "    df['lf'] = df['lf'].astype('int')\n",
    "    df = reorder_columns(df, ['nct_id', 'lf'])\n",
    "    return df\n",
    "           \n",
    "def lf_gpt(path='./Trial Linkage/pubmed_gpt_outcomes.csv'):\n",
    "    df = pd.read_csv(path)\n",
    "    df['outcome'].unique()\n",
    "    df['lf'] = -1\n",
    "    df.loc[df['outcome']=='Success', ['lf']] = 1\n",
    "    # df.loc[df['outcome']=='Not sure',['lf']] = 1\n",
    "    # df.loc[df['outcome']=='Not sure',['lf']] = -1\n",
    "    df.loc[df['outcome']=='Not sure',['lf']] = 0\n",
    "    df.loc[df['outcome']=='Failure', ['lf']] = 0\n",
    "    df.rename(columns={'nctid': 'nct_id'}, inplace=True)           \n",
    "    df = reorder_columns(df, ['nct_id', 'lf'])\n",
    "    return df\n",
    "\n",
    "def get_lfs(path='./CITT/'):\n",
    "    dfs = [\\\n",
    "        lf_results_reported(path=path),\n",
    "        lf_num_sponsors(path=path),\n",
    "        lf_num_patients(path=path), \n",
    "        lf_patient_drop(path=path), \n",
    "        lf_sites(path=path), \n",
    "        lf_pvalues(path=path),\n",
    "        lf_pvalues(path=path),\n",
    "        lf_update_more_recent(path=path),\n",
    "        lf_death_ae(path=path),\n",
    "        lf_serious_ae(path=path),\n",
    "        lf_all_ae(path=path),\n",
    "        lf_status(path=path),\n",
    "        lf_status(path=path),\n",
    "        lf_amendments(),\n",
    "        lf_stock_price(),\n",
    "        lf_linkage(),\n",
    "        lf_linkage(),\n",
    "        lf_linkage(),\n",
    "        lf_news_headlines(),\n",
    "        lf_gpt(),\n",
    "        lf_gpt(),\n",
    "        ]\n",
    "\n",
    "    all_ids = set() # set of all nct_ids\n",
    "    for df in dfs:\n",
    "        all_ids = all_ids | set(df['nct_id'])\n",
    "\n",
    "    all_df = pd.DataFrame(all_ids, columns=['nct_id']) # combine all dfs\n",
    "    for i, df in enumerate(dfs):\n",
    "        all_df = pd.merge(all_df, df.iloc[:,:2].rename(columns={'lf':'lf'+str(i)}), on='nct_id', how='left')\n",
    "    all_df = all_df.fillna(-1)\n",
    "    # all_df.iloc[:,1:] = all_df.iloc[:,1:].astype('int')\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3167782/2588819231.py:109: DtypeWarning: Columns (46,47,48,53,68,69) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path+'studies.txt', sep='|')\n",
      "/tmp/ipykernel_3167782/2588819231.py:109: DtypeWarning: Columns (46,47,48,53,68,69) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path+'studies.txt', sep='|')\n",
      "/tmp/ipykernel_3167782/2588819231.py:143: DtypeWarning: Columns (53,68) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    5031\n",
      "0    4019\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "df = get_lfs()\n",
    "\n",
    "# classification report\n",
    "path = './clinical-trial-outcome-prediction/data/'\n",
    "all_files = glob.glob(os.path.join(path, \"phase*train.csv\")) + glob.glob(os.path.join(path, \"phase*valid.csv\"))\n",
    "hint = pd.concat((pd.read_csv(f) for f in all_files))\n",
    "hint.rename(columns={'nctid': 'nct_id'}, inplace=True)\n",
    "print(hint['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 62/200 [00:06<00:14,  9.24epoch/s]"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import LabelModel, MajorityLabelVoter\n",
    "from scipy.stats import mode\n",
    "# L = np.array([[0, 0, -1], [-1, 0, 1], [1, -1, 0]])\n",
    "# Y_dev = [0, 1, 0]\n",
    "df2 = df.copy()\n",
    "L = df2.iloc[:,1:].values.astype('int')\n",
    "label_model = LabelModel(verbose=False, cardinality=2)\n",
    "# label_model = MajorityLabelVoter(cardinality=2)\n",
    "# label_model.fit(L)\n",
    "# label_model.fit(L, Y_dev=Y_dev, seed=2020, lr=0.05)\n",
    "# label_model.fit(L, class_balance=[0.44, 0.56], seed=0)\n",
    "# label_model.fit(L, class_balance=[.5, .5], seed=0, lr=0.05, n_epochs=100)\n",
    "positive_prop = .21\n",
    "\n",
    "# all_preds = []\n",
    "# for seed in range(5):\n",
    "#     label_model.fit(L, class_balance=[1-positive_prop, positive_prop], seed=seed, lr=0.002, n_epochs=200)\n",
    "#     all_preds.append(label_model.predict(L))\n",
    "# pred = mode(all_preds, axis=0)[0]\n",
    "\n",
    "# label_model.fit(L, class_balance=[1-positive_prop, positive_prop], seed=0, lr=0.002, n_epochs=200)\n",
    "label_model.fit(L, class_balance=[1-positive_prop, positive_prop], seed=0, lr=0.002, n_epochs=200)\n",
    "pred = label_model.predict(L)\n",
    "df2['pred'] = pred.astype('int')\n",
    "# df2 = pd.read_csv('lfs.csv')\n",
    "# df2 = pd.read_csv('lfs2.csv')\n",
    "# pred = mode(np.stack([df2['pred'].values, df['pred'].values], axis=0), axis=0)[0] \n",
    "# df2['pred'] = pred\n",
    "\n",
    "print('predicted label distribution', np.unique(df2['pred'], return_counts=True))\n",
    "print('label balance on top', np.unique(pd.merge(hint, df2, on='nct_id', how='left')['pred'], return_counts=True))\n",
    "\n",
    "for phase in ['phase 1', 'phase 2', 'phase 3']:\n",
    "    hint_subset = hint[hint['phase'].str.contains(phase)]\n",
    "    combined = pd.merge(hint_subset, df2, on='nct_id', how='left')\n",
    "    combined = combined.dropna(subset=['pred'])\n",
    "    combined = combined[combined['pred'] != -1]\n",
    "    print(phase, hint_subset.shape, combined.shape)\n",
    "    report = classification_report(combined['label'], combined['pred'], output_dict=True)\n",
    "    print('1 class', report['1'])\n",
    "    print('0 class', report['0'])\n",
    "# df.to_csv('lfs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('lfs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for phase in ['phase 1', 'phase 2', 'phase 3']:\n",
    "    hint_subset = hint[hint['phase'].str.contains(phase)]\n",
    "    combined = pd.merge(hint_subset, df2, on='nct_id', how='left')\n",
    "    combined = combined.dropna(subset=['pred'])\n",
    "    combined = combined[combined['pred'] != -1]\n",
    "    print(phase, hint_subset.shape, combined.shape)\n",
    "    report = classification_report(combined['label'], combined['pred'], output_dict=True)\n",
    "    print(report['1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3167782/2951246534.py:109: DtypeWarning: Columns (46,47,48,53,68,69) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path+'studies.txt', sep='|')\n",
      "/tmp/ipykernel_3167782/2951246534.py:143: DtypeWarning: Columns (53,68) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 16\n",
      "lf, -1.0, 0.0, 1.0, prop, coverage, 1_f1_phase1, 1_f1_phase2, 1_f1_phase3, 0_f1_phase1, 0_f1_phase2, 0_f1_phase3\n",
      "results_reported,0,415614,61624,0.12912634785997762,0.9947411315217368,0.4097693351424695,0.4935972060535506,0.5292740046838408,0.5181781861765881,0.6899189918991899,0.5060931899641578,\n",
      "num_sponsors,0,321091,156147,0.3271889497483436,0.9947411315217368,0.3671562082777036,0.44037780401416765,0.3663663663663664,0.5271504912946043,0.31468740652108884,0.4112053456694937,\n",
      "num_patients,0,30831,30793,0.49969167856679214,0.12844728937950353,0.5118279569892473,0.680731364275668,0.6337186129932244,0.680791941646405,0.8369757826343769,0.34285714285714286,\n",
      "patient_drop,0,19191,18554,0.4915617962644059,0.07867459005629887,0.4494949494949495,0.3433734939759036,0.41302027748132336,0.434737923946557,0.20604565083281925,0.38273381294964026,\n",
      "sites,0,313131,113484,0.26601033718926903,0.8892240094547077,0.5554106910039114,0.5290055248618785,0.5344544708777687,0.49622725255215266,0.6318114874815906,0.3267504488330341,\n",
      "pvalues,0,10855,10386,0.4889600301304082,0.04427412815964616,0.6046511627906976,0.5641025641025641,0.7015945330296127,0.661498708010336,0.8345323741007195,0.6101694915254238,\n",
      "update_more_recent,16868,230601,229769,0.4990963789995004,0.9595819585168448,0.24072216649949849,0.6159309994926433,0.16450537236013338,0.6672568983325955,0.07663157894736843,0.5014776085473971,\n",
      "death_ae,0,10326,51094,0.8318788668186259,0.1280220776595013,0.6804374240583232,0.254957507082153,0.6614826334888543,0.14304461942257218,0.8326738270209157,0.11641791044776119,\n",
      "serious_ae,0,29213,32207,0.524373168349072,0.1280220776595013,0.3408624229979466,0.5341074020319303,0.3043062200956938,0.5583232077764277,0.1946607341490545,0.3991701244813278,\n",
      "all_ae,0,30569,30851,0.5022956691631391,0.1280220776595013,0.2684310018903592,0.401854714064915,0.19141914191419143,0.4740877031585403,0.1344055140723722,0.3891366031617349,\n",
      "status,432907,44136,195,0.004398727752588482,0.09240225862460684,NA,NA,NA,\n",
      "amendments,0,10989,8895,0.4473445986722993,0.041445636473160595,0.6253229974160207,0.2564102564102564,0.6265625,0.3626666666666667,0.6993693062368606,0.27899159663865547,\n",
      "stock_price,0,9386,10498,0.527962180647757,0.041445636473160595,0.6699507389162561,0.23863636363636365,0.5755274261603376,0.40473372781065087,0.6524064171122995,0.36185133239831696,\n",
      "linkage,0,116695,35355,0.23252219664584017,0.3169286373840308,0.5378151260504201,0.4918265813788202,0.5589270008795075,0.5909461663947798,0.6984960611124373,0.5083690151810043,\n",
      "news_headlines,0,43687,43687,0.5,0.18211984717390534,0.5258711721224921,0.5027685492801772,0.5542246681774037,0.5590778097982709,0.6522210184182016,0.41530054644808745,\n",
      "gpt,519,93642,69831,0.4271714595070746,0.3407384093329804,0.7280575539568346,0.6037735849056604,0.7625106022052587,0.7227722772277227,0.8449511400651466,0.7076167076167076,\n"
     ]
    }
   ],
   "source": [
    "path = './CITT/'\n",
    "funcs = [\\\n",
    "        lf_results_reported(path=path),\n",
    "        lf_num_sponsors(path=path),\n",
    "        lf_num_patients(path=path), \n",
    "        lf_patient_drop(path=path), \n",
    "        lf_sites(path=path), \n",
    "        lf_pvalues(path=path),\n",
    "        lf_update_more_recent(path=path),\n",
    "        lf_death_ae(path=path),\n",
    "        lf_serious_ae(path=path),\n",
    "        lf_all_ae(path=path),\n",
    "        lf_status(path=path),\n",
    "        lf_amendments(),\n",
    "        lf_stock_price(),\n",
    "        lf_linkage(),\n",
    "        lf_news_headlines(),\n",
    "        lf_gpt(),\n",
    "    ]\n",
    "names = ['results_reported', 'num_sponsors', 'num_patients', 'patient_drop', 'sites', 'pvalues', 'update_more_recent', 'death_ae', 'serious_ae', 'all_ae', 'status', 'amendments', 'stock_price', 'linkage', 'news_headlines', 'gpt']\n",
    "\n",
    "print(len(funcs), len(names))\n",
    "# all trials stats\n",
    "\n",
    "print('lf, -1.0, 0.0, 1.0, prop, coverage, 1_f1_phase1, 1_f1_phase2, 1_f1_phase3, 0_f1_phase1, 0_f1_phase2, 0_f1_phase3')\n",
    "for i in range(len(funcs)):\n",
    "    value_counts = funcs[i]['lf'].value_counts()\n",
    "    value_dict = value_counts.to_dict()\n",
    "\n",
    "    print(names[i], end=',')\n",
    "    for key in [-1.0, 0.0, 1.0]:\n",
    "        if key not in value_dict:\n",
    "            value_dict[key] = 0\n",
    "        print(value_dict[key], end=',')\n",
    "\n",
    "    print(value_dict[1.0] / (value_dict[1.0] + value_dict[0.0]), end=',')\n",
    "    print(sum([value_dict[k] for k in value_dict.keys() if k!=-1.0]) / len(df),end=',')\n",
    "\n",
    "    # agreement with HINT phase 1,2,3\n",
    "    for phase in ['phase 1', 'phase 2', 'phase 3']:\n",
    "        hint_subset = hint[hint['phase'].str.contains(phase)]\n",
    "        combined = pd.merge(hint_subset, funcs[i], on='nct_id', how='left')\n",
    "        combined = combined[combined['lf'] != -1].dropna(subset=['lf'])\n",
    "        # print(combined['lf'].value_counts())\n",
    "        # if all values are same, skip\n",
    "        if len(combined['lf'].unique()) == 1:\n",
    "            print('NA', end=',')\n",
    "            continue\n",
    "        report = classification_report(combined['label'], combined['lf'].astype(int), output_dict=True)\n",
    "        print(report['1']['f1-score'], end=',')\n",
    "        print(report['0']['f1-score'], end=',')\n",
    "    print()\n",
    "# studies = pd.read_csv('./CITT/studies.txt', sep='|')\n",
    "# studies = studies[studies['study_type'] == 'Interventional']\n",
    "# studies.completion_date = pd.to_datetime(studies.completion_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ get top sponsors ============\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = './CITT/'\n",
    "sponsors = pd.read_csv(data_path + 'sponsors.txt', sep='|')\n",
    "studies = pd.read_csv(data_path + 'studies.txt', sep='|', low_memory=False)\n",
    "ticker_df = pd.read_csv('stock_price/ticker_dict_642.csv')\n",
    "\n",
    "studies['study_first_submitted_date'] = pd.to_datetime(studies['study_first_submitted_date'])\n",
    "sponsors = pd.merge(sponsors, studies[['nct_id', 'phase', 'study_first_submitted_date']], on='nct_id', how='left')\n",
    "sponsors = sponsors[sponsors['agency_class']=='INDUSTRY']\n",
    "sponsors.dropna(inplace=True)\n",
    "sponsors = sponsors[sponsors['phase'].str.contains('Phase 3')]\n",
    "# len(sponsors[sponsors['agency_class']=='INDUSTRY']['name'].unique()) #15277\n",
    "\n",
    "# top sponsors and their cumulated trial coverage\n",
    "num_sponsors = []\n",
    "coverage = []\n",
    "for i in range(100, len(sponsors['name'].unique()), 100):\n",
    "    top_sponsors = sponsors['name'].value_counts().head(i)\n",
    "    coverage_ = top_sponsors.sum() / sponsors['name'].value_counts().sum()\n",
    "    num_sponsors.append(i)\n",
    "    coverage.append(coverage_)\n",
    "plt.scatter(num_sponsors, coverage, label='Cumulated trial coverage')\n",
    "plt.xlabel('Number of top sponsors')\n",
    "plt.ylabel('Cumulated trial coverage')\n",
    "plt.grid()\n",
    "\n",
    "top_sponsors = sponsors['name'].value_counts().head(1000)\n",
    "coverage_ = top_sponsors.sum() / sponsors['name'].value_counts().sum()\n",
    "combined = pd.merge(top_sponsors.reset_index(), sponsors.groupby('name')['study_first_submitted_date'].min().reset_index(), on='name', how='left')\n",
    "combined = pd.merge(combined, ticker_df, on='name', how='left')\n",
    "print(combined.isna().sum())\n",
    "print(combined.head())\n",
    "# combined.to_csv('top_sponsors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# with zipfile.ZipFile('./fda_nat_drug_code_dir/ndctext.zip') as zf:\n",
    "#     print(zf.namelist())\n",
    "#     with zf.open(\"product.txt\", mode=\"r\") as file:\n",
    "#         product = pd.read_csv(file, sep='\\t', encoding='unicode_escape')\n",
    "#     with zf.open(\"package.txt\", mode=\"r\") as file:\n",
    "#         package = pd.read_csv(file, sep='\\t', encoding='unicode_escape')\n",
    "# with zipfile.ZipFile('./fda_nat_drug_code_dir/ndc_unfinished.zip') as zf:\n",
    "#     print(zf.namelist())\n",
    "#     with zf.open(\"unfinished_product.txt\", mode=\"r\") as file:\n",
    "#         product2 = pd.read_csv(file, sep='\\t', encoding='unicode_escape')\n",
    "#     with zf.open(\"unfinished_package.txt\", mode=\"r\") as file:\n",
    "#         package2 = pd.read_csv(file, sep='\\t', encoding='unicode_escape')\n",
    "# with zipfile.ZipFile('./fda_nat_drug_code_dir/ndc_excluded.zip') as zf:\n",
    "#     print(zf.namelist())\n",
    "#     with zf.open(\"Products_excluded.txt\", mode=\"r\") as file:\n",
    "#         product3 = pd.read_csv(file, sep='\\t', encoding='unicode_escape')\n",
    "#     with zf.open(\"Packages_excluded.txt\", mode=\"r\") as file:\n",
    "#         package3 = pd.read_csv(file, sep='\\t', encoding='unicode_escape')\n",
    "# with zipfile.ZipFile('./fda_nat_drug_code_dir/compounders_ndc_directory.zip') as zf:\n",
    "#     print(zf.namelist())\n",
    "#     with zf.open(\"compounders_ndc_directory.txt\", mode=\"r\") as file:\n",
    "#         product4 = pd.read_csv(file, sep='\\t', encoding='unicode_escape')\n",
    "\n",
    "with zipfile.ZipFile('./openfda/drug-ndc-0001-of-0001.json.zip') as zf:\n",
    "    with zf.open(\"drug-ndc-0001-of-0001.json\") as f:\n",
    "        data = json.load(f)\n",
    "prod_ndc_dict = pd.DataFrame(data['results']).astype(str)\n",
    "orange_book = pd.read_csv('./EOBZIP_2024_04/products.txt', sep='~').astype(str)\n",
    "\n",
    "# remove nonnumeric from application number\n",
    "prod_ndc_dict['application_number'] = prod_ndc_dict['application_number'].str.replace(r'\\D', '', regex=True)\n",
    "\n",
    "# results = ['Marketing Category, Total, # In Orange Book']\n",
    "print('Marketing Category, Total, # In Orange Book')\n",
    "for marketing_category in prod_ndc_dict['marketing_category'].unique():\n",
    "    a = set(prod_ndc_dict[prod_ndc_dict['marketing_category']==marketing_category]['application_number'])\n",
    "    # check lenth of overlap with orange book\n",
    "    l = len([a_ for a_ in a if a_ in set(orange_book['Appl_No'])])\n",
    "    print(f\"{marketing_category}, {len(prod_ndc_dict[prod_ndc_dict['marketing_category']==marketing_category])}, {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================== overlap between drugbank and orangebook ==================\n",
    "# import pandas as pd\n",
    "# # df = pd.read_csv('./drugbank_all_drug_links.csv.zip')\n",
    "# drugbank_approved = pd.read_csv('./drugbank/drugbank_approved_drug_links.csv.zip')\n",
    "\n",
    "# # exclusivity = pd.read_csv('./EOBZIP_2024_02/exclusivity.txt', sep='~')\n",
    "# # patent = pd.read_csv('./EOBZIP_2024_02/patent.txt', sep='~')\n",
    "# products = pd.read_csv('./EOBZIP_2024_02/products.txt', sep='~')\n",
    "# drug_vocab = pd.read_csv('./drugbank/drugbank vocabulary.csv')\n",
    "\n",
    "# drugbank_approved['Name'] = drugbank_approved['Name'].str.lower()\n",
    "# drugbank_approved_names = set(drugbank_approved['Name'].unique())\n",
    "\n",
    "# products[['Ingredient', 'Trade_Name']] = products[['Ingredient', 'Trade_Name']].apply(lambda x: x.str.lower().str.strip())\n",
    "# products['Ingredient'] = products['Ingredient'].str.split(';')\n",
    "# products['Ingredient'] = products['Ingredient'].apply(lambda x: [y.strip() for y in x] if isinstance(x, list) else [])\n",
    "# unique_ingredients = set(products['Ingredient'].explode().unique())\n",
    "\n",
    "# products['Trade_Name'] = products['Trade_Name'].str.split(' and ')\n",
    "# products['Trade_Name'] = products['Trade_Name'].apply(lambda x: [y.strip() for y in x] if isinstance(x, list) else [])\n",
    "# unique_trade_names = set(products['Trade_Name'].explode().unique())\n",
    "\n",
    "# drug_vocab['Synonyms'] = drug_vocab['Synonyms'].str.split('|')\n",
    "# drug_vocab['Synonyms'] = drug_vocab['Synonyms'].apply(lambda x: [y.lower().strip() for y in x] if isinstance(x, list) else [])\n",
    "\n",
    "# all_synonyms = set(drug_vocab['Synonyms'].explode().dropna().unique())\n",
    "# common_names = set(drug_vocab['Common name'].str.lower().unique())\n",
    "# all_names = common_names.union(all_synonyms)\n",
    "\n",
    "# print('unique_ingredients', len(unique_ingredients)) # 2264\n",
    "# print(len(unique_ingredients.intersection(all_synonyms))) # 1088\n",
    "# print(len(unique_ingredients.intersection(common_names))) # 1095\n",
    "# print(len(unique_ingredients.intersection(all_names))) # 1183\n",
    "\n",
    "# print('unique_trade_names', len(unique_trade_names)) # 7071\n",
    "# print(len(unique_trade_names.intersection(all_synonyms))) # 669\n",
    "# print(len(unique_trade_names.intersection(common_names))) # 629\n",
    "# print(len(unique_trade_names.intersection(all_names))) # 701\n",
    "\n",
    "# print('drugbank_approved_names', len(drugbank_approved_names)) # 4389\n",
    "# print(len(drugbank_approved_names.intersection(unique_ingredients))) # 1074\n",
    "# print(len(drugbank_approved_names.intersection(unique_trade_names))) # 624"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('./GNews/')\n",
    "# from gnews import GNews\n",
    "\n",
    "# google_news = GNews()\n",
    "# pakistan_news = google_news.get_news('Pakistan')\n",
    "# print(pakistan_news[0])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yfinance as yf\n",
    "\n",
    "# msft = yf.Ticker(\"MSFT\")\n",
    "\n",
    "# # get all stock info\n",
    "# msft.info\n",
    "\n",
    "# # get historical market data\n",
    "# hist = msft.history(period=\"1mo\")\n",
    "\n",
    "# # show meta information about the history (requires history() to be called first)\n",
    "# msft.history_metadata\n",
    "\n",
    "# # show actions (dividends, splits, capital gains)\n",
    "# msft.actions\n",
    "# msft.dividends\n",
    "# msft.splits\n",
    "# msft.capital_gains  # only for mutual funds & etfs\n",
    "\n",
    "# # show share count\n",
    "# msft.get_shares_full(start=\"2022-01-01\", end=None)\n",
    "\n",
    "# # show financials:\n",
    "# # - income statement\n",
    "# msft.income_stmt\n",
    "# msft.quarterly_income_stmt\n",
    "# # - balance sheet\n",
    "# msft.balance_sheet\n",
    "# msft.quarterly_balance_sheet\n",
    "# # - cash flow statement\n",
    "# msft.cashflow\n",
    "# msft.quarterly_cashflow\n",
    "# # see `Ticker.get_income_stmt()` for more options\n",
    "\n",
    "# # # show holders\n",
    "# # msft.major_holders\n",
    "# # msft.institutional_holders\n",
    "# # msft.mutualfund_holders\n",
    "# # msft.insider_transactions\n",
    "# # msft.insider_purchases\n",
    "# # msft.insider_roster_holders\n",
    "\n",
    "# # # show recommendations\n",
    "# # msft.recommendations\n",
    "# # msft.recommendations_summary\n",
    "# # msft.upgrades_downgrades\n",
    "\n",
    "# # Show future and historic earnings dates, returns at most next 4 quarters and last 8 quarters by default. \n",
    "# # Note: If more are needed use msft.get_earnings_dates(limit=XX) with increased limit argument.\n",
    "# msft.earnings_dates\n",
    "\n",
    "# # # show ISIN code - *experimental*\n",
    "# # # ISIN = International Securities Identification Number\n",
    "# # msft.isin\n",
    "\n",
    "# # # show options expirations\n",
    "# # msft.options\n",
    "\n",
    "# # # show news\n",
    "# # msft.news\n",
    "\n",
    "# # # get option chain for specific expiration\n",
    "# # opt = msft.option_chain('YYYY-MM-DD')\n",
    "# # # data available via: opt.calls, opt.puts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests # doesn't work\n",
    "# def get_ticker(company_name):\n",
    "#     res = requests.get(url=\"https://query2.finance.yahoo.com/v1/finance/search\", \n",
    "#                        params={\n",
    "#                            \"q\": company_name, \n",
    "#                            }, \n",
    "#                        headers={\n",
    "#                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "#                            }\n",
    "#                            )\n",
    "#     data = res.json()\n",
    "#     return data\n",
    "#     # print(data)\n",
    "#     # company_code = data['quotes'][0]['symbol']\n",
    "#     # return company_code\n",
    "\n",
    "# data = get_ticker('')\n",
    "# data.keys() #dict_keys(['explains', 'count', 'quotes', 'news', 'nav', 'lists', 'researchReports', 'screenerFieldResults', 'totalTime', 'timeTakenForQuotes', 'timeTakenForNews', 'timeTakenForAlgowatchlist', 'timeTakenForPredefinedScreener', 'timeTakenForCrunchbase', 'timeTakenForNav', 'timeTakenForResearchReports', 'timeTakenForScreenerField', 'timeTakenForCulturalAssets'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # with open(\"./tmp/NCT00102336_pubmed_abs.json\", 'r') as f:\n",
    "# with open(\"./tmp/NCT00102336_pubmed_abs.json\", 'r') as f:\n",
    "#     d = json.load(f)\n",
    "\n",
    "# # print(len(d['References']))\n",
    "# # print(d['References'][0].keys())\n",
    "# # for i in range(len(d['References'])):\n",
    "# #     print(i, d['References'][i]['Reference type'])\n",
    "\n",
    "# prompt = '''\n",
    "# You are given the PubMed abstract for a clinical trial. Your task is to use summarize important values of the trial into a json format. After summarization, you must predict the trial outcome.\n",
    "\n",
    "# Guidelines:\n",
    "# - **Completeness**: Ensure there are no missing statistical tests and descriptions in json output.\n",
    "# - **Data Verification**: Before concluding the final answer, always verify that your observations align with the original trial description. Do not create any new information.\n",
    "\n",
    "# Output Format:\n",
    "# {\n",
    "#     \"description\": <string of text summary of the trial outcome>,\n",
    "#     \"extracted features\": [\n",
    "#         {\n",
    "#         \"description: <string, text describing the feature extracted: e.g. \"platelet response\", \"number of participants\", \"confidence interval\", \"p-value\", \"number\", \"study design\">\n",
    "#         \"value\": <float or string of the values of above description>\n",
    "#         }, ... # can repeat as many times as needed\n",
    "#     ]\n",
    "#     \"outcome\": <string, must either \"succeed\", \"fail\", \"unsure\">,\n",
    "#     \"outcome reasoning\": <string, reasoning as to why you predicted the outcome. Most trials succeed if the primary p-value < 0.05>\n",
    "# }\n",
    "\n",
    "# Notes for final output:\n",
    "# - Ensure the final answer format is a valid json dictionary. \n",
    "# - Ensure all \"value\" are of float or string ONLY  \n",
    "\n",
    "# Here is the abstract: \n",
    "\n",
    "# [ABSTRACT]\n",
    "\n",
    "# Begin!\n",
    "# '''\n",
    "\n",
    "# abs = d['References'][5]['Abstract']\n",
    "\n",
    "# print(prompt.replace(\"[ABSTRACT]\", abs))\n",
    "# # print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distilltab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
